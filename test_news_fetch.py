#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—æ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
Discord Botã‚’ä½¿ã‚ãšã«ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—æ©Ÿèƒ½ã ã‘ã‚’ãƒ†ã‚¹ãƒˆã—ã¾ã™
"""

import re
import json
import logging
from pathlib import Path
from typing import List, Dict, Optional
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# è¨­å®š
NEWS_URL = "https://www.tenkaippin.co.jp/news/"
TOKYO_KEYWORDS = [
    "æ±äº¬", "éƒ½å†…", "æ–°å®¿", "æ¸‹è°·", "æ± è¢‹", "ä¸Šé‡", "å“å·", "ç›®é»’", "ä¸–ç”°è°·",
    "å¤§ç”°", "æ‰ä¸¦", "ç·´é¦¬", "æ¿æ©‹", "åŒ—åŒº", "è’å·", "å°æ±", "å¢¨ç”°", "æ±Ÿæ±",
    "ä¸­å¤®", "åƒä»£ç”°", "æ¸¯åŒº", "æ–‡äº¬", "è¶³ç«‹", "è‘›é£¾", "æ±Ÿæˆ¸å·", "å…«ç‹å­",
    "ç«‹å·", "æ­¦è”µé‡", "ä¸‰é·¹", "åºœä¸­", "èª¿å¸ƒ", "ç”ºç”°", "å°é‡‘äº•", "å°å¹³",
    "æ—¥é‡", "æ±æ‘å±±", "å›½åˆ†å¯º", "å›½ç«‹", "ç¦ç”Ÿ", "ç‹›æ±Ÿ", "æ±å¤§å’Œ", "æ¸…ç€¬",
    "æ±ä¹…ç•™ç±³", "æ­¦è”µæ‘å±±", "å¤šæ‘©", "ç¨²åŸ", "ç¾½æ‘", "ã‚ãã‚‹é‡", "è¥¿æ±äº¬",
    "23åŒº", "æ±äº¬éƒ½"
]


class TenkaippinCrawler:
    """å¤©ä¸‹ä¸€å“ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒšãƒ¼ã‚¸ã®ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def fetch_news(self) -> List[Dict]:
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒšãƒ¼ã‚¸ã‹ã‚‰è¨˜äº‹ä¸€è¦§ã‚’å–å¾—"""
        try:
            response = self.session.get(NEWS_URL, timeout=10)
            response.raise_for_status()
            response.encoding = response.apparent_encoding
            
            soup = BeautifulSoup(response.text, 'html.parser')
            news_items = []
            
            # ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’æŠ½å‡ºï¼ˆãƒšãƒ¼ã‚¸æ§‹é€ ã«å¿œã˜ã¦èª¿æ•´ãŒå¿…è¦ãªå ´åˆã‚ã‚Šï¼‰
            # æ—¥ä»˜ã¨ã‚¿ã‚¤ãƒˆãƒ«ã‚’å«ã‚€è¦ç´ ã‚’æ¢ã™
            news_elements = soup.find_all(['li', 'div', 'article'], class_=re.compile(r'news|item|entry', re.I))
            
            # ã‚‚ã—ç‰¹å®šã®ã‚¯ãƒ©ã‚¹ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€ã‚ˆã‚Šåºƒç¯„å›²ã«æ¤œç´¢
            if not news_elements:
                # æ—¥ä»˜ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆYYYY.MM.DDå½¢å¼ï¼‰ã‚’å«ã‚€è¦ç´ ã‚’æ¢ã™
                date_pattern = re.compile(r'\d{4}\.\d{2}\.\d{2}')
                for element in soup.find_all(text=date_pattern):
                    parent = element.find_parent()
                    if parent:
                        news_elements.append(parent)
            
            for element in news_elements:
                try:
                    # æ—¥ä»˜ã‚’æŠ½å‡º
                    date_text = element.get_text()
                    date_match = re.search(r'(\d{4})\.(\d{2})\.(\d{2})', date_text)
                    if not date_match:
                        continue
                    
                    date_str = f"{date_match.group(1)}-{date_match.group(2)}-{date_match.group(3)}"
                    
                    # ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º
                    title_elem = element.find(['a', 'h3', 'h2', 'h4'])
                    if not title_elem:
                        # ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º
                        title_text = element.get_text(strip=True)
                        # æ—¥ä»˜éƒ¨åˆ†ã‚’é™¤ã„ãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¿ã‚¤ãƒˆãƒ«ã¨ã™ã‚‹
                        title = re.sub(r'\d{4}\.\d{2}\.\d{2}\s*', '', title_text).strip()
                    else:
                        title = title_elem.get_text(strip=True)
                    
                    # URLã‚’æŠ½å‡º
                    link_elem = element.find('a', href=True)
                    if link_elem:
                        url = urljoin(NEWS_URL, link_elem['href'])
                    else:
                        url = NEWS_URL
                    
                    if title:
                        news_items.append({
                            'date': date_str,
                            'title': title,
                            'url': url,
                            'text': element.get_text(strip=True)
                        })
                
                except Exception as e:
                    logger.warning(f"è¨˜äº‹ã®è§£æä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
            # é‡è¤‡ã‚’é™¤å»
            seen_titles = set()
            unique_items = []
            for item in news_items:
                if item['title'] not in seen_titles:
                    seen_titles.add(item['title'])
                    unique_items.append(item)
            
            logger.info(f"{len(unique_items)}ä»¶ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’å–å¾—ã—ã¾ã—ãŸ")
            return unique_items
            
        except Exception as e:
            logger.error(f"ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def fetch_article_detail(self, url: str) -> Optional[str]:
        """è¨˜äº‹è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰æœ¬æ–‡ã‚’å–å¾—"""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            response.encoding = response.apparent_encoding
            
            soup = BeautifulSoup(response.text, 'html.parser')
            # æœ¬æ–‡ã‚’å–å¾—ï¼ˆä¸€èˆ¬çš„ãªè¨˜äº‹æœ¬æ–‡ã®ã‚»ãƒ¬ã‚¯ã‚¿ã‚’è©¦ã™ï¼‰
            content_selectors = [
                'article', '.article', '.content', '.post-content',
                '.entry-content', 'main', '.main-content'
            ]
            
            for selector in content_selectors:
                content = soup.select_one(selector)
                if content:
                    return content.get_text(strip=True)
            
            # ã‚»ãƒ¬ã‚¯ã‚¿ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯bodyå…¨ä½“ã‹ã‚‰å–å¾—
            body = soup.find('body')
            if body:
                return body.get_text(strip=True)
            
            return None
        except Exception as e:
            logger.warning(f"è¨˜äº‹è©³ç´°ã®å–å¾—ã‚¨ãƒ©ãƒ¼ ({url}): {e}")
            return None
    
    def extract_address_from_text(self, text: str) -> Optional[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ä½æ‰€æƒ…å ±ã‚’æŠ½å‡º"""
        if not text:
            return None
        
        # éƒµä¾¿ç•ªå·ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆã€’123-4567 ã¾ãŸã¯ 123-4567ï¼‰
        postal_pattern = r'[ã€’]?\d{3}-?\d{4}'
        
        # éƒ½é“åºœçœŒãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæ±äº¬éƒ½ã€å¤§é˜ªåºœãªã©ï¼‰
        prefecture_pattern = r'[éƒ½é“åºœçœŒ]+'
        
        # ä½æ‰€ã‚‰ã—ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¢ã™ï¼ˆéƒµä¾¿ç•ªå·ã®å‰å¾Œã€éƒ½é“åºœçœŒã®å‰å¾Œï¼‰
        # éƒµä¾¿ç•ªå·ã®å‰å¾Œ100æ–‡å­—ç¨‹åº¦ã‚’æŠ½å‡º
        postal_matches = list(re.finditer(postal_pattern, text))
        for match in postal_matches:
            start = max(0, match.start() - 50)
            end = min(len(text), match.end() + 200)
            address_candidate = text[start:end]
            if 'æ±äº¬éƒ½' in address_candidate or 'æ±äº¬' in address_candidate:
                return address_candidate
        
        # éƒ½é“åºœçœŒãƒ‘ã‚¿ãƒ¼ãƒ³ã§æ¤œç´¢
        prefecture_matches = list(re.finditer(prefecture_pattern, text))
        for match in prefecture_matches:
            start = max(0, match.start() - 20)
            end = min(len(text), match.end() + 100)
            address_candidate = text[start:end]
            if 'æ±äº¬éƒ½' in address_candidate:
                return address_candidate
        
        # ã€Œæ±äº¬éƒ½ã€ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ç›´æ¥ãƒã‚§ãƒƒã‚¯
        if 'æ±äº¬éƒ½' in text:
            # ã€Œæ±äº¬éƒ½ã€ã®å‰å¾Œã‚’æŠ½å‡º
            tokyo_index = text.find('æ±äº¬éƒ½')
            if tokyo_index != -1:
                start = max(0, tokyo_index - 20)
                end = min(len(text), tokyo_index + 100)
                return text[start:end]
        
        return None
    
    def extract_opening_date(self, text: str) -> Optional[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚ªãƒ¼ãƒ—ãƒ³æ—¥ã‚’æŠ½å‡º"""
        if not text:
            return None
        
        # ã‚ªãƒ¼ãƒ—ãƒ³æ—¥é–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆå„ªå…ˆé †ä½é †ï¼‰
        opening_keywords = ['ã‚ªãƒ¼ãƒ—ãƒ³æ—¥ï¼š', 'ã‚ªãƒ¼ãƒ—ãƒ³æ—¥', 'é–‹åº—æ—¥ï¼š', 'é–‹åº—æ—¥', 'ã‚ªãƒ¼ãƒ—ãƒ³', 'é–‹åº—']
        
        # æ—¥ä»˜ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆYYYYå¹´MMæœˆDDæ—¥(æ›œæ—¥)ã‚’å«ã‚€ï¼‰
        # ã€Œ2025å¹´11æœˆ17æ—¥(æœˆ)ã€ã®ã‚ˆã†ãªå½¢å¼ã«å¯¾å¿œ
        date_patterns = [
            r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥(?:\([æœˆç«æ°´æœ¨é‡‘åœŸæ—¥]\))?',  # 2025å¹´11æœˆ17æ—¥(æœˆ) å½¢å¼
            r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',  # 2025å¹´11æœˆ17æ—¥ å½¢å¼
            r'(\d{4})/(\d{1,2})/(\d{1,2})',
            r'(\d{4})\.(\d{1,2})\.(\d{1,2})',
            r'(\d{4})-(\d{1,2})-(\d{1,2})',
            r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})',
        ]
        
        # å„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å‘¨è¾ºã‚’æ¤œç´¢ï¼ˆã€Œã‚ªãƒ¼ãƒ—ãƒ³æ—¥ï¼šã€ã®ã‚ˆã†ãªæ˜ç¢ºãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å„ªå…ˆï¼‰
        for keyword in opening_keywords:
            keyword_index = text.find(keyword)
            if keyword_index != -1:
                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å¾Œã‚300æ–‡å­—ã‚’æŠ½å‡ºï¼ˆå‰ã¯ä¸è¦ï¼‰
                start = keyword_index + len(keyword)
                end = min(len(text), start + 300)
                context = text[start:end]
                
                # æ—¥ä»˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ç›´å¾Œã«ã‚ã‚‹æ—¥ä»˜ã‚’å„ªå…ˆï¼‰
                for pattern in date_patterns:
                    match = re.search(pattern, context)
                    if match:
                        year, month, day = match.groups()[:3]  # æœ€åˆã®3ã¤ã®ã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆå¹´ã€æœˆã€æ—¥ï¼‰ã‚’å–å¾—
                        # YYYY-MM-DDå½¢å¼ã«çµ±ä¸€
                        return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€ã€Œã‚ªãƒ¼ãƒ—ãƒ³æ—¥ã€ã¨ã„ã†æ–‡å­—åˆ—ã®å‘¨è¾ºã‚’æ¤œç´¢
        if 'ã‚ªãƒ¼ãƒ—ãƒ³æ—¥' in text or 'é–‹åº—æ—¥' in text:
            # ã‚ˆã‚Šåºƒç¯„å›²ã§æ¤œç´¢
            for pattern in date_patterns:
                matches = list(re.finditer(pattern, text))
                if matches:
                    # æœ€åˆã«è¦‹ã¤ã‹ã£ãŸæ—¥ä»˜ã‚’è¿”ã™
                    match = matches[0]
                    year, month, day = match.groups()[:3]
                    return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
        
        return None
    
    def is_tokyo_store(self, news_item: Dict) -> bool:
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒéƒ½å†…ã®æ–°åº—æƒ…å ±ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        title = news_item.get('title', '')
        text = news_item.get('text', '')
        combined_text = f"{title} {text}"
        
        # æ–°åº—é–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯
        store_keywords = ['ã‚ªãƒ¼ãƒ—ãƒ³', 'é–‹åº—', 'æ–°åº—', 'åº—èˆ—', 'åº—']
        has_store_keyword = any(keyword in combined_text for keyword in store_keywords)
        
        if not has_store_keyword:
            return False
        
        # ã¾ãšã€ã‚¿ã‚¤ãƒˆãƒ«ãƒ»æœ¬æ–‡ã«éƒ½å†…é–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        for keyword in TOKYO_KEYWORDS:
            if keyword in combined_text:
                return True
        
        # ã‚¿ã‚¤ãƒˆãƒ«ãƒ»æœ¬æ–‡ã«éƒ½å†…ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒãªã„å ´åˆã€è©³ç´°ãƒšãƒ¼ã‚¸ã‚’ãƒã‚§ãƒƒã‚¯
        url = news_item.get('url')
        if url and url != NEWS_URL:
            logger.info(f"è©³ç´°ãƒšãƒ¼ã‚¸ã‚’ãƒã‚§ãƒƒã‚¯: {title}")
            detail_text = self.fetch_article_detail(url)
            if detail_text:
                # è©³ç´°ãƒšãƒ¼ã‚¸ã®ãƒ†ã‚­ã‚¹ãƒˆã‚‚å«ã‚ã¦åˆ¤å®š
                full_text = f"{combined_text} {detail_text}"
                
                # éƒ½å†…ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å†ãƒã‚§ãƒƒã‚¯ï¼ˆè©³ç´°ãƒšãƒ¼ã‚¸ã®ãƒ†ã‚­ã‚¹ãƒˆã‚‚å«ã‚€ï¼‰
                for keyword in TOKYO_KEYWORDS:
                    if keyword in full_text:
                        # ã‚ªãƒ¼ãƒ—ãƒ³æ—¥ã‚’æŠ½å‡ºã—ã¦news_itemã«è¿½åŠ 
                        opening_date = self.extract_opening_date(detail_text)
                        if opening_date:
                            news_item['opening_date'] = opening_date
                        return True
                
                # ä½æ‰€æƒ…å ±ã‹ã‚‰åˆ¤å®š
                address = self.extract_address_from_text(detail_text)
                if address:
                    if 'æ±äº¬éƒ½' in address:
                        logger.info(f"ä½æ‰€æƒ…å ±ã‹ã‚‰éƒ½å†…ã¨åˆ¤å®š: {address[:50]}...")
                        # ã‚ªãƒ¼ãƒ—ãƒ³æ—¥ã‚’æŠ½å‡ºã—ã¦news_itemã«è¿½åŠ 
                        opening_date = self.extract_opening_date(detail_text)
                        if opening_date:
                            news_item['opening_date'] = opening_date
                        return True
        
        return False


def test_fetch_news():
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ã®ãƒ†ã‚¹ãƒˆ"""
    print("=" * 60)
    print("å¤©ä¸‹ä¸€å“ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ãƒ†ã‚¹ãƒˆ")
    print("=" * 60)
    
    crawler = TenkaippinCrawler()
    
    print("\n[1] ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸€è¦§ã®å–å¾—ã‚’é–‹å§‹...")
    news_items = crawler.fetch_news()
    
    if not news_items:
        print("âŒ ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return
    
    print(f"âœ… {len(news_items)}ä»¶ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’å–å¾—ã—ã¾ã—ãŸ\n")
    
    # æœ€æ–°5ä»¶ã‚’è¡¨ç¤º
    print("[2] æœ€æ–°5ä»¶ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹:")
    print("-" * 60)
    for i, item in enumerate(news_items[:5], 1):
        print(f"\n{i}. æ—¥ä»˜: {item['date']}")
        print(f"   ã‚¿ã‚¤ãƒˆãƒ«: {item['title']}")
        print(f"   URL: {item['url']}")
        print(f"   æœ¬æ–‡ï¼ˆæœ€åˆã®100æ–‡å­—ï¼‰: {item['text'][:100]}...")
    
    # æ–°åº—é–¢é€£ã®è¨˜äº‹ã‚’æŠ½å‡º
    print("\n\n[3] æ–°åº—é–¢é€£ã®è¨˜äº‹ã‚’æŠ½å‡º:")
    print("-" * 60)
    store_keywords = ['ã‚ªãƒ¼ãƒ—ãƒ³', 'é–‹åº—', 'æ–°åº—', 'åº—èˆ—', 'åº—']
    store_news = []
    
    for item in news_items:
        combined_text = f"{item['title']} {item['text']}"
        if any(keyword in combined_text for keyword in store_keywords):
            store_news.append(item)
    
    print(f"âœ… æ–°åº—é–¢é€£ã®è¨˜äº‹: {len(store_news)}ä»¶\n")
    for i, item in enumerate(store_news[:5], 1):
        print(f"{i}. {item['date']} - {item['title']}")
    
    # éƒ½å†…ã®æ–°åº—æƒ…å ±ã‚’åˆ¤å®š
    print("\n\n[4] éƒ½å†…ã®æ–°åº—æƒ…å ±ã‚’åˆ¤å®š:")
    print("-" * 60)
    tokyo_stores = []
    
    for item in store_news:
        # è©³ç´°ãƒšãƒ¼ã‚¸ã‚’å–å¾—ã—ã¦ã‚ªãƒ¼ãƒ—ãƒ³æ—¥ã‚’æŠ½å‡ºï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
        url = item.get('url')
        if url and url != NEWS_URL:
            detail_text = crawler.fetch_article_detail(url)
            if detail_text:
                # ã‚ªãƒ¼ãƒ—ãƒ³æ—¥ã‚’æŠ½å‡º
                opening_date = crawler.extract_opening_date(detail_text)
                if opening_date:
                    item['opening_date'] = opening_date
                    print(f"   [DEBUG] ã‚ªãƒ¼ãƒ—ãƒ³æ—¥ã‚’æŠ½å‡º: {opening_date}")
                else:
                    # ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šè©³ç´°ãƒšãƒ¼ã‚¸ã®ä¸€éƒ¨ã‚’è¡¨ç¤º
                    if 'ã‚ªãƒ¼ãƒ—ãƒ³æ—¥' in detail_text:
                        idx = detail_text.find('ã‚ªãƒ¼ãƒ—ãƒ³æ—¥')
                        snippet = detail_text[max(0, idx-20):min(len(detail_text), idx+100)]
                        print(f"   [DEBUG] ã‚ªãƒ¼ãƒ—ãƒ³æ—¥å‘¨è¾ºã®ãƒ†ã‚­ã‚¹ãƒˆ: {snippet}")
        
        if crawler.is_tokyo_store(item):
            tokyo_stores.append(item)
    
    print(f"âœ… éƒ½å†…ã®æ–°åº—æƒ…å ±: {len(tokyo_stores)}ä»¶\n")
    for i, item in enumerate(tokyo_stores, 1):
        print(f"{i}. {item['date']} - {item['title']}")
        print(f"   URL: {item['url']}")
        
        # ã‚ªãƒ¼ãƒ—ãƒ³æ—¥ãŒã‚ã‚‹å ´åˆã¯è¡¨ç¤º
        opening_date = item.get('opening_date')
        if opening_date:
            print(f"   ğŸ—“ï¸  ã‚ªãƒ¼ãƒ—ãƒ³æ—¥: {opening_date}")
        
        # è©³ç´°ãƒšãƒ¼ã‚¸ã‚’ãƒã‚§ãƒƒã‚¯ã—ãŸå ´åˆã¯ãã®æƒ…å ±ã‚‚è¡¨ç¤º
        if item['url'] != NEWS_URL:
            print(f"   (è©³ç´°ãƒšãƒ¼ã‚¸ã‚ã‚Š)")
    
    # çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    output_file = Path(__file__).parent / "test_results.json"
    results = {
        "total_news": len(news_items),
        "store_news": len(store_news),
        "tokyo_stores": len(tokyo_stores),
        "news_items": news_items[:10],  # æœ€æ–°10ä»¶
        "store_news_items": store_news[:10],
        "tokyo_store_items": tokyo_stores
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\n\n[5] çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã—ãŸ: {output_file}")
    print("=" * 60)
    print("ãƒ†ã‚¹ãƒˆå®Œäº†ï¼")
    print("=" * 60)

if __name__ == "__main__":
    try:
        test_fetch_news()
    except KeyboardInterrupt:
        print("\n\nãƒ†ã‚¹ãƒˆãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
