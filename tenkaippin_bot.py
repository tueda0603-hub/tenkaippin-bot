#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤©ä¸‹ä¸€å“ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ & Discord Bot
å¤©ä¸‹ä¸€å“ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒšãƒ¼ã‚¸ã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã€éƒ½å†…ã®æ–°åº—æƒ…å ±ã‚’Discordã«æŠ•ç¨¿ã™ã‚‹
"""

import os
import re
import json
import asyncio
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Optional
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup
import discord
from discord.ext import tasks
from dotenv import load_dotenv

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('tenkaippin_bot.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

# è¨­å®š
NEWS_URL = "https://www.tenkaippin.co.jp/news/"
HISTORY_FILE = Path("posted_history.json")
# ãƒã‚§ãƒƒã‚¯ã™ã‚‹æ—¥ä»˜ç¯„å›²ï¼ˆæ—¥æ•°ï¼‰ã€‚ã“ã®æ—¥æ•°ä»¥å†…ã®è¨˜äº‹ã®ã¿ã‚’å‡¦ç†
DAYS_TO_CHECK = int(os.getenv("DAYS_TO_CHECK", "7"))  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ7æ—¥é–“
# æŠ•ç¨¿å±¥æ­´ã®ä¿æŒæœŸé–“ï¼ˆæ—¥æ•°ï¼‰ã€‚ã“ã®æœŸé–“ã‚’è¶…ãˆãŸå±¥æ­´ã¯è‡ªå‹•å‰Šé™¤
HISTORY_RETENTION_DAYS = int(os.getenv("HISTORY_RETENTION_DAYS", "90"))  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ90æ—¥é–“
TOKYO_KEYWORDS = [
    "æ±äº¬", "éƒ½å†…", "æ–°å®¿", "æ¸‹è°·", "æ± è¢‹", "ä¸Šé‡", "å“å·", "ç›®é»’", "ä¸–ç”°è°·",
    "å¤§ç”°", "æ‰ä¸¦", "ç·´é¦¬", "æ¿æ©‹", "åŒ—åŒº", "è’å·", "å°æ±", "å¢¨ç”°", "æ±Ÿæ±",
    "ä¸­å¤®", "åƒä»£ç”°", "æ¸¯åŒº", "æ–‡äº¬", "è¶³ç«‹", "è‘›é£¾", "æ±Ÿæˆ¸å·", "å…«ç‹å­",
    "ç«‹å·", "æ­¦è”µé‡", "ä¸‰é·¹", "åºœä¸­", "èª¿å¸ƒ", "ç”ºç”°", "å°é‡‘äº•", "å°å¹³",
    "æ—¥é‡", "æ±æ‘å±±", "å›½åˆ†å¯º", "å›½ç«‹", "ç¦ç”Ÿ", "ç‹›æ±Ÿ", "æ±å¤§å’Œ", "æ¸…ç€¬",
    "æ±ä¹…ç•™ç±³", "æ­¦è”µæ‘å±±", "å¤šæ‘©", "ç¨²åŸ", "ç¾½æ‘", "ã‚ãã‚‹é‡", "è¥¿æ±äº¬",
    "23åŒº", "æ±äº¬éƒ½"
]

# Discordè¨­å®š
DISCORD_TOKEN = os.getenv("DISCORD_TOKEN")
DISCORD_CHANNEL_ID = int(os.getenv("DISCORD_CHANNEL_ID", "0"))


class TenkaippinCrawler:
    """å¤©ä¸‹ä¸€å“ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒšãƒ¼ã‚¸ã®ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def fetch_news(self) -> List[Dict]:
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒšãƒ¼ã‚¸ã‹ã‚‰è¨˜äº‹ä¸€è¦§ã‚’å–å¾—"""
        try:
            response = self.session.get(NEWS_URL, timeout=10)
            response.raise_for_status()
            response.encoding = response.apparent_encoding
            
            soup = BeautifulSoup(response.text, 'html.parser')
            news_items = []
            
            # ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’æŠ½å‡ºï¼ˆãƒšãƒ¼ã‚¸æ§‹é€ ã«å¿œã˜ã¦èª¿æ•´ãŒå¿…è¦ãªå ´åˆã‚ã‚Šï¼‰
            # æ—¥ä»˜ã¨ã‚¿ã‚¤ãƒˆãƒ«ã‚’å«ã‚€è¦ç´ ã‚’æ¢ã™
            news_elements = soup.find_all(['li', 'div', 'article'], class_=re.compile(r'news|item|entry', re.I))
            
            # ã‚‚ã—ç‰¹å®šã®ã‚¯ãƒ©ã‚¹ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€ã‚ˆã‚Šåºƒç¯„å›²ã«æ¤œç´¢
            if not news_elements:
                # æ—¥ä»˜ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆYYYY.MM.DDå½¢å¼ï¼‰ã‚’å«ã‚€è¦ç´ ã‚’æ¢ã™
                date_pattern = re.compile(r'\d{4}\.\d{2}\.\d{2}')
                for element in soup.find_all(text=date_pattern):
                    parent = element.find_parent()
                    if parent:
                        news_elements.append(parent)
            
            for element in news_elements:
                try:
                    # æ—¥ä»˜ã‚’æŠ½å‡º
                    date_text = element.get_text()
                    date_match = re.search(r'(\d{4})\.(\d{2})\.(\d{2})', date_text)
                    if not date_match:
                        continue
                    
                    date_str = f"{date_match.group(1)}-{date_match.group(2)}-{date_match.group(3)}"
                    
                    # ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º
                    title_elem = element.find(['a', 'h3', 'h2', 'h4'])
                    if not title_elem:
                        # ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º
                        title_text = element.get_text(strip=True)
                        # æ—¥ä»˜éƒ¨åˆ†ã‚’é™¤ã„ãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¿ã‚¤ãƒˆãƒ«ã¨ã™ã‚‹
                        title = re.sub(r'\d{4}\.\d{2}\.\d{2}\s*', '', title_text).strip()
                    else:
                        title = title_elem.get_text(strip=True)
                    
                    # URLã‚’æŠ½å‡º
                    link_elem = element.find('a', href=True)
                    if link_elem:
                        url = urljoin(NEWS_URL, link_elem['href'])
                    else:
                        url = NEWS_URL
                    
                    if title:
                        news_items.append({
                            'date': date_str,
                            'title': title,
                            'url': url,
                            'text': element.get_text(strip=True)
                        })
                
                except Exception as e:
                    logger.warning(f"è¨˜äº‹ã®è§£æä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
            # é‡è¤‡ã‚’é™¤å»
            seen_titles = set()
            unique_items = []
            for item in news_items:
                if item['title'] not in seen_titles:
                    seen_titles.add(item['title'])
                    unique_items.append(item)
            
            logger.info(f"{len(unique_items)}ä»¶ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’å–å¾—ã—ã¾ã—ãŸ")
            return unique_items
            
        except Exception as e:
            logger.error(f"ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def fetch_article_detail(self, url: str) -> Optional[str]:
        """è¨˜äº‹è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰æœ¬æ–‡ã‚’å–å¾—"""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            response.encoding = response.apparent_encoding
            
            soup = BeautifulSoup(response.text, 'html.parser')
            # æœ¬æ–‡ã‚’å–å¾—ï¼ˆä¸€èˆ¬çš„ãªè¨˜äº‹æœ¬æ–‡ã®ã‚»ãƒ¬ã‚¯ã‚¿ã‚’è©¦ã™ï¼‰
            content_selectors = [
                'article', '.article', '.content', '.post-content',
                '.entry-content', 'main', '.main-content'
            ]
            
            for selector in content_selectors:
                content = soup.select_one(selector)
                if content:
                    return content.get_text(strip=True)
            
            # ã‚»ãƒ¬ã‚¯ã‚¿ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯bodyå…¨ä½“ã‹ã‚‰å–å¾—
            body = soup.find('body')
            if body:
                return body.get_text(strip=True)
            
            return None
        except Exception as e:
            logger.warning(f"è¨˜äº‹è©³ç´°ã®å–å¾—ã‚¨ãƒ©ãƒ¼ ({url}): {e}")
            return None
    
    def extract_address_from_text(self, text: str) -> Optional[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ä½æ‰€æƒ…å ±ã‚’æŠ½å‡º"""
        if not text:
            return None
        
        # éƒµä¾¿ç•ªå·ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆã€’123-4567 ã¾ãŸã¯ 123-4567ï¼‰
        postal_pattern = r'[ã€’]?\d{3}-?\d{4}'
        
        # éƒ½é“åºœçœŒãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæ±äº¬éƒ½ã€å¤§é˜ªåºœãªã©ï¼‰
        prefecture_pattern = r'[éƒ½é“åºœçœŒ]+'
        
        # ä½æ‰€ã‚‰ã—ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¢ã™ï¼ˆéƒµä¾¿ç•ªå·ã®å‰å¾Œã€éƒ½é“åºœçœŒã®å‰å¾Œï¼‰
        # éƒµä¾¿ç•ªå·ã®å‰å¾Œ100æ–‡å­—ç¨‹åº¦ã‚’æŠ½å‡º
        postal_matches = list(re.finditer(postal_pattern, text))
        for match in postal_matches:
            start = max(0, match.start() - 50)
            end = min(len(text), match.end() + 200)
            address_candidate = text[start:end]
            if 'æ±äº¬éƒ½' in address_candidate or 'æ±äº¬' in address_candidate:
                return address_candidate
        
        # éƒ½é“åºœçœŒãƒ‘ã‚¿ãƒ¼ãƒ³ã§æ¤œç´¢
        prefecture_matches = list(re.finditer(prefecture_pattern, text))
        for match in prefecture_matches:
            start = max(0, match.start() - 20)
            end = min(len(text), match.end() + 100)
            address_candidate = text[start:end]
            if 'æ±äº¬éƒ½' in address_candidate:
                return address_candidate
        
        # ã€Œæ±äº¬éƒ½ã€ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ç›´æ¥ãƒã‚§ãƒƒã‚¯
        if 'æ±äº¬éƒ½' in text:
            # ã€Œæ±äº¬éƒ½ã€ã®å‰å¾Œã‚’æŠ½å‡º
            tokyo_index = text.find('æ±äº¬éƒ½')
            if tokyo_index != -1:
                start = max(0, tokyo_index - 20)
                end = min(len(text), tokyo_index + 100)
                return text[start:end]
        
        return None
    
    def extract_opening_date(self, text: str) -> Optional[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚ªãƒ¼ãƒ—ãƒ³æ—¥ã‚’æŠ½å‡º"""
        if not text:
            return None
        
        # ã‚ªãƒ¼ãƒ—ãƒ³æ—¥é–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆå„ªå…ˆé †ä½é †ï¼‰
        opening_keywords = ['ã‚ªãƒ¼ãƒ—ãƒ³æ—¥ï¼š', 'ã‚ªãƒ¼ãƒ—ãƒ³æ—¥', 'é–‹åº—æ—¥ï¼š', 'é–‹åº—æ—¥', 'ã‚ªãƒ¼ãƒ—ãƒ³', 'é–‹åº—']
        
        # æ—¥ä»˜ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆYYYYå¹´MMæœˆDDæ—¥(æ›œæ—¥)ã‚’å«ã‚€ï¼‰
        # ã€Œ2025å¹´11æœˆ17æ—¥(æœˆ)ã€ã®ã‚ˆã†ãªå½¢å¼ã«å¯¾å¿œ
        date_patterns = [
            r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥(?:\([æœˆç«æ°´æœ¨é‡‘åœŸæ—¥]\))?',  # 2025å¹´11æœˆ17æ—¥(æœˆ) å½¢å¼
            r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',  # 2025å¹´11æœˆ17æ—¥ å½¢å¼
            r'(\d{4})/(\d{1,2})/(\d{1,2})',
            r'(\d{4})\.(\d{1,2})\.(\d{1,2})',
            r'(\d{4})-(\d{1,2})-(\d{1,2})',
            r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})',
        ]
        
        # å„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å‘¨è¾ºã‚’æ¤œç´¢ï¼ˆã€Œã‚ªãƒ¼ãƒ—ãƒ³æ—¥ï¼šã€ã®ã‚ˆã†ãªæ˜ç¢ºãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å„ªå…ˆï¼‰
        for keyword in opening_keywords:
            keyword_index = text.find(keyword)
            if keyword_index != -1:
                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å¾Œã‚300æ–‡å­—ã‚’æŠ½å‡ºï¼ˆå‰ã¯ä¸è¦ï¼‰
                start = keyword_index + len(keyword)
                end = min(len(text), start + 300)
                context = text[start:end]
                
                # æ—¥ä»˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ç›´å¾Œã«ã‚ã‚‹æ—¥ä»˜ã‚’å„ªå…ˆï¼‰
                for pattern in date_patterns:
                    match = re.search(pattern, context)
                    if match:
                        year, month, day = match.groups()[:3]  # æœ€åˆã®3ã¤ã®ã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆå¹´ã€æœˆã€æ—¥ï¼‰ã‚’å–å¾—
                        # YYYY-MM-DDå½¢å¼ã«çµ±ä¸€
                        return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€ã€Œã‚ªãƒ¼ãƒ—ãƒ³æ—¥ã€ã¨ã„ã†æ–‡å­—åˆ—ã®å‘¨è¾ºã‚’æ¤œç´¢
        if 'ã‚ªãƒ¼ãƒ—ãƒ³æ—¥' in text or 'é–‹åº—æ—¥' in text:
            # ã‚ˆã‚Šåºƒç¯„å›²ã§æ¤œç´¢
            for pattern in date_patterns:
                matches = list(re.finditer(pattern, text))
                if matches:
                    # æœ€åˆã«è¦‹ã¤ã‹ã£ãŸæ—¥ä»˜ã‚’è¿”ã™
                    match = matches[0]
                    year, month, day = match.groups()[:3]
                    return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
        
        return None
    
    def is_tokyo_store(self, news_item: Dict) -> bool:
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒéƒ½å†…ã®æ–°åº—æƒ…å ±ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        title = news_item.get('title', '')
        text = news_item.get('text', '')
        combined_text = f"{title} {text}"
        
        # æ–°åº—é–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯
        store_keywords = ['ã‚ªãƒ¼ãƒ—ãƒ³', 'é–‹åº—', 'æ–°åº—', 'åº—èˆ—', 'åº—']
        has_store_keyword = any(keyword in combined_text for keyword in store_keywords)
        
        if not has_store_keyword:
            return False
        
        # ã¾ãšã€ã‚¿ã‚¤ãƒˆãƒ«ãƒ»æœ¬æ–‡ã«éƒ½å†…é–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        for keyword in TOKYO_KEYWORDS:
            if keyword in combined_text:
                return True
        
        # ã‚¿ã‚¤ãƒˆãƒ«ãƒ»æœ¬æ–‡ã«éƒ½å†…ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒãªã„å ´åˆã€è©³ç´°ãƒšãƒ¼ã‚¸ã‚’ãƒã‚§ãƒƒã‚¯
        url = news_item.get('url')
        if url and url != NEWS_URL:
            logger.info(f"è©³ç´°ãƒšãƒ¼ã‚¸ã‚’ãƒã‚§ãƒƒã‚¯: {title}")
            detail_text = self.fetch_article_detail(url)
            if detail_text:
                # è©³ç´°ãƒšãƒ¼ã‚¸ã®ãƒ†ã‚­ã‚¹ãƒˆã‚‚å«ã‚ã¦åˆ¤å®š
                full_text = f"{combined_text} {detail_text}"
                
                # éƒ½å†…ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å†ãƒã‚§ãƒƒã‚¯ï¼ˆè©³ç´°ãƒšãƒ¼ã‚¸ã®ãƒ†ã‚­ã‚¹ãƒˆã‚‚å«ã‚€ï¼‰
                for keyword in TOKYO_KEYWORDS:
                    if keyword in full_text:
                        # ã‚ªãƒ¼ãƒ—ãƒ³æ—¥ã‚’æŠ½å‡ºã—ã¦news_itemã«è¿½åŠ 
                        opening_date = self.extract_opening_date(detail_text)
                        if opening_date:
                            news_item['opening_date'] = opening_date
                        return True
                
                # ä½æ‰€æƒ…å ±ã‹ã‚‰åˆ¤å®š
                address = self.extract_address_from_text(detail_text)
                if address:
                    if 'æ±äº¬éƒ½' in address:
                        logger.info(f"ä½æ‰€æƒ…å ±ã‹ã‚‰éƒ½å†…ã¨åˆ¤å®š: {address[:50]}...")
                        # ã‚ªãƒ¼ãƒ—ãƒ³æ—¥ã‚’æŠ½å‡ºã—ã¦news_itemã«è¿½åŠ 
                        opening_date = self.extract_opening_date(detail_text)
                        if opening_date:
                            news_item['opening_date'] = opening_date
                        return True
        
        return False


class HistoryManager:
    """æŠ•ç¨¿å±¥æ­´ã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, history_file: Path, retention_days: int = 90):
        self.history_file = history_file
        self.retention_days = retention_days
        self.history = self.load_history()
        self.cleanup_old_history()
    
    def load_history(self) -> Dict[str, str]:
        """æŠ•ç¨¿å±¥æ­´ã‚’èª­ã¿è¾¼ã‚€ï¼ˆkey: è¨˜äº‹ã®ã‚­ãƒ¼, value: æŠ•ç¨¿æ—¥æ™‚ã®ISOå½¢å¼ï¼‰"""
        if self.history_file.exists():
            try:
                with open(self.history_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    # æ—§å½¢å¼ï¼ˆsetï¼‰ã¨ã®äº’æ›æ€§ã‚’ä¿ã¤
                    posted_items = data.get('posted_items', [])
                    if isinstance(posted_items, list):
                        if posted_items and isinstance(posted_items[0], str):
                            # æ—§å½¢å¼ï¼šæ–‡å­—åˆ—ã®ãƒªã‚¹ãƒˆ â†’ æ–°å½¢å¼ã«å¤‰æ›
                            history = {}
                            for item in posted_items:
                                history[item] = datetime.now().isoformat()
                            return history
                        else:
                            # æ–°å½¢å¼ï¼šè¾æ›¸å½¢å¼
                            return data.get('history', {})
                    return {}
            except Exception as e:
                logger.warning(f"å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return {}
    
    def save_history(self):
        """æŠ•ç¨¿å±¥æ­´ã‚’ä¿å­˜ã™ã‚‹"""
        try:
            data = {
                'last_updated': datetime.now().isoformat(),
                'history': self.history,
                'retention_days': self.retention_days
            }
            with open(self.history_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def cleanup_old_history(self):
        """å¤ã„æŠ•ç¨¿å±¥æ­´ã‚’å‰Šé™¤"""
        cutoff_date = datetime.now() - timedelta(days=self.retention_days)
        initial_count = len(self.history)
        
        keys_to_remove = []
        for key, posted_at_str in self.history.items():
            try:
                posted_at = datetime.fromisoformat(posted_at_str)
                if posted_at < cutoff_date:
                    keys_to_remove.append(key)
            except (ValueError, TypeError):
                # ç„¡åŠ¹ãªæ—¥ä»˜å½¢å¼ã®å ´åˆã¯å‰Šé™¤
                keys_to_remove.append(key)
        
        for key in keys_to_remove:
            del self.history[key]
        
        if keys_to_remove:
            logger.info(f"å¤ã„æŠ•ç¨¿å±¥æ­´ã‚’{len(keys_to_remove)}ä»¶å‰Šé™¤ã—ã¾ã—ãŸï¼ˆ{initial_count}ä»¶ â†’ {len(self.history)}ä»¶ï¼‰")
            self.save_history()
    
    def is_posted(self, news_item: Dict) -> bool:
        """æ—¢ã«æŠ•ç¨¿æ¸ˆã¿ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯"""
        # ã‚¿ã‚¤ãƒˆãƒ«ã¨æ—¥ä»˜ã®çµ„ã¿åˆã‚ã›ã§ãƒ¦ãƒ‹ãƒ¼ã‚¯æ€§ã‚’åˆ¤å®š
        key = f"{news_item.get('date')}_{news_item.get('title')}"
        return key in self.history
    
    def mark_as_posted(self, news_item: Dict):
        """æŠ•ç¨¿æ¸ˆã¿ã¨ã—ã¦ãƒãƒ¼ã‚¯"""
        key = f"{news_item.get('date')}_{news_item.get('title')}"
        self.history[key] = datetime.now().isoformat()
        self.save_history()


class DiscordBot(discord.Client):
    """Discord Bot"""
    
    def __init__(self, channel_id: int):
        intents = discord.Intents.default()
        super().__init__(intents=intents)
        self.channel_id = channel_id
        self.crawler = TenkaippinCrawler()
        self.history_manager = HistoryManager(HISTORY_FILE, HISTORY_RETENTION_DAYS)
    
    async def on_ready(self):
        """BotãŒèµ·å‹•ã—ãŸã¨ãã®å‡¦ç†"""
        logger.info(f'{self.user}ã¨ã—ã¦ãƒ­ã‚°ã‚¤ãƒ³ã—ã¾ã—ãŸ')
        # æ¯æ—¥ã®ã‚¯ãƒ­ãƒ¼ãƒ«ã‚’é–‹å§‹
        self.daily_crawl.start()
    
    @tasks.loop(hours=24)
    async def daily_crawl(self):
        """æ¯æ—¥ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦æŠ•ç¨¿"""
        await self.crawl_and_post()
    
    @daily_crawl.before_loop
    async def before_daily_crawl(self):
        """åˆå›å®Ÿè¡Œå‰ã«å¾…æ©Ÿ"""
        await self.wait_until_ready()
        # èµ·å‹•æ™‚ã«ã‚‚ä¸€åº¦å®Ÿè¡Œ
        await self.crawl_and_post()
    
    def filter_recent_news(self, news_items: List[Dict], days: int) -> List[Dict]:
        """æŒ‡å®šæ—¥æ•°ä»¥å†…ã®è¨˜äº‹ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        cutoff_date = datetime.now() - timedelta(days=days)
        filtered_items = []
        
        for item in news_items:
            date_str = item.get('date', '')
            try:
                # æ—¥ä»˜æ–‡å­—åˆ—ã‚’ãƒ‘ãƒ¼ã‚¹ï¼ˆYYYY-MM-DDå½¢å¼ã‚’æƒ³å®šï¼‰
                item_date = datetime.strptime(date_str, '%Y-%m-%d')
                if item_date >= cutoff_date:
                    filtered_items.append(item)
            except (ValueError, TypeError) as e:
                logger.warning(f"æ—¥ä»˜ã®ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {date_str} - {e}")
                # æ—¥ä»˜ãŒãƒ‘ãƒ¼ã‚¹ã§ããªã„å ´åˆã¯å«ã‚ãªã„ï¼ˆå®‰å…¨ã®ãŸã‚ï¼‰
                continue
        
        logger.info(f"æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°: {len(news_items)}ä»¶ â†’ {len(filtered_items)}ä»¶ï¼ˆç›´è¿‘{days}æ—¥ä»¥å†…ï¼‰")
        return filtered_items
    
    async def crawl_and_post(self):
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦éƒ½å†…ã®æ–°åº—æƒ…å ±ã‚’æŠ•ç¨¿"""
        try:
            logger.info("ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã‚¯ãƒ­ãƒ¼ãƒ«ã‚’é–‹å§‹ã—ã¾ã™...")
            news_items = self.crawler.fetch_news()
            
            if not news_items:
                logger.warning("ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                return
            
            # ç›´è¿‘Næ—¥ä»¥å†…ã®è¨˜äº‹ã®ã¿ã‚’å‡¦ç†
            recent_news = self.filter_recent_news(news_items, DAYS_TO_CHECK)
            
            if not recent_news:
                logger.info(f"ç›´è¿‘{DAYS_TO_CHECK}æ—¥ä»¥å†…ã®è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return
            
            # éƒ½å†…ã®æ–°åº—æƒ…å ±ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆæŠ•ç¨¿å±¥æ­´ã‚‚ãƒã‚§ãƒƒã‚¯ï¼‰
            tokyo_stores = [
                item for item in recent_news
                if self.crawler.is_tokyo_store(item) and not self.history_manager.is_posted(item)
            ]
            
            if not tokyo_stores:
                logger.info("éƒ½å†…ã®æ–°åº—æƒ…å ±ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return
            
            # Discordãƒãƒ£ãƒ³ãƒãƒ«ã«æŠ•ç¨¿
            channel = self.get_channel(self.channel_id)
            if not channel:
                logger.error(f"ãƒãƒ£ãƒ³ãƒãƒ«ID {self.channel_id} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return
            
            for store_info in tokyo_stores:
                embed = discord.Embed(
                    title="ğŸª å¤©ä¸‹ä¸€å“ éƒ½å†…æ–°åº—æƒ…å ±",
                    description=store_info['title'],
                    url=store_info['url'],
                    color=discord.Color.orange(),
                    timestamp=datetime.now()
                )
                embed.add_field(name="è¨˜äº‹æ—¥ä»˜", value=store_info['date'], inline=True)
                
                # ã‚ªãƒ¼ãƒ—ãƒ³æ—¥ãŒã‚ã‚‹å ´åˆã¯è¡¨ç¤º
                opening_date = store_info.get('opening_date')
                if opening_date:
                    embed.add_field(name="ã‚ªãƒ¼ãƒ—ãƒ³æ—¥", value=opening_date, inline=True)
                
                embed.add_field(name="è©³ç´°", value=f"[è¨˜äº‹ã‚’èª­ã‚€]({store_info['url']})", inline=True)
                
                await channel.send(embed=embed)
                self.history_manager.mark_as_posted(store_info)
                logger.info(f"æŠ•ç¨¿ã—ã¾ã—ãŸ: {store_info['title']}")
                
                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’é¿ã‘ã‚‹ãŸã‚å°‘ã—å¾…æ©Ÿ
                await asyncio.sleep(1)
        
        except Exception as e:
            logger.error(f"ã‚¯ãƒ­ãƒ¼ãƒ«ãƒ»æŠ•ç¨¿å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    if not DISCORD_TOKEN:
        logger.error("DISCORD_TOKENãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return
    
    if DISCORD_CHANNEL_ID == 0:
        logger.error("DISCORD_CHANNEL_IDãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return
    
    bot = DiscordBot(DISCORD_CHANNEL_ID)
    bot.run(DISCORD_TOKEN)


if __name__ == "__main__":
    main()
